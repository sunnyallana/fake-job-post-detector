# -*- coding: utf-8 -*-
"""Sequential_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_GdT5ttEwyHrYUds2KSipBRiIQbuOQE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# import torch.tensor as torch
import torch

df = pd.read_excel('fake_job_posting.xlsx')
df.shape

df.head(5)

sns.countplot(x='fraudulent', data=df, palette='Set1')
plt.title('Distribution of Fraudulent Job Postings')
plt.xlabel('Fraudulent')
plt.ylabel('Count')
plt.show()
df['fraudulent'].value_counts(normalize=True)

df.duplicated().sum()

df = df.drop_duplicates()

text_columns = ['title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits']
df[text_columns] = df[text_columns].fillna("")

df["text"] = df[text_columns].apply(lambda x: ' '.join(str(i) for i in x), axis=1)

df = df[["text", "fraudulent"]]

import re
# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])

def clean_text(text):
    text = text.lower()
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = remove_stopwords(text)
    return text

df['text']

df['text'] = df['text'].apply(clean_text)

df['text']

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'], df['fraudulent'], test_size=0.2, random_state=42, stratify=df['fraudulent']
)

from torch.utils.data import Dataset
from collections import Counter



def tokenize(text):
    return re.findall(r'\b\w+\b', text.lower())

counter = Counter()
for i in train_texts:
    counter.update(tokenize(i))

vocab_size = 20000
vocab = {"<PAD>": 0, "<UNK>": 1}
for i, (word, _) in enumerate(counter.most_common(vocab_size - 2), start=2):
    vocab[word] = i

def encode(text, max_len=1024):
    tokens = tokenize(text)
    token_ids = [vocab.get(token, vocab["<UNK>"]) for token in tokens[:max_len]]
    if len(token_ids) > max_len:
        token_ids = token_ids[:max_len]
    else:
        token_ids += [vocab["<PAD>"]] * (max_len - len(token_ids))
    return token_ids

class JobDataset(Dataset):

    def __init__(self, texts, labels):
        self.texts = [torch.tensor(encode(text), dtype=torch.long) for text in texts]
        self.labels = torch.tensor(labels.values if hasattr(labels, 'values') else labels, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]

from torch.utils.data import DataLoader
batch_size = 64
t_ds = JobDataset(train_texts, train_labels)
v_ds = JobDataset(val_texts, val_labels)
train_loader = DataLoader(t_ds, batch_size=batch_size, shuffle=True, pin_memory=True)
val_loader = DataLoader(v_ds, batch_size=batch_size, shuffle=False, pin_memory=True)

def load_glove_embeddings(glove_path, vocal, embed_dim=100):
    embeddings_index = {}

    with open(glove_path, encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vec = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = vec

    embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), embed_dim))
    for word, idx in vocab.items():
        if word in embeddings_index:
            embedding_matrix[idx] = embeddings_index[word]
    return torch.tensor(embedding_matrix, dtype=torch.float32)

import torch.nn as nn
from tqdm.notebook import tqdm
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

import torch.nn as nn, torch

class FakeJobLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128,
                 num_layers=1, dropout=0.3):
        super().__init__()
        glove_path = 'glove.6B.100d.txt'
        embedding_matrix = load_glove_embeddings(
            glove_path, vocab, embed_dim)
        self.embedding = nn.Embedding.from_pretrained(
            embedding_matrix, freeze=False, padding_idx=0)

        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,
                            batch_first=True, bidirectional=True, dropout=dropout)

        self.dropout = nn.Dropout(dropout)
        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim * 2)
        self.fc_out = nn.Linear(hidden_dim * 2, 1)

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        pooled, _ = torch.max(lstm_out, dim=1)
        x = self.dropout(pooled)
        x = torch.relu(self.fc_hidden(x))
        x = self.fc_out(x)
        return x.squeeze(1)

model = FakeJobLSTM(vocab_size=len(vocab)).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def train_model(model, train_loader, val_loader, epochs=50):
    train_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        print(f"\nEpoch {epoch + 1}/{epochs}")
        train_loop = tqdm(train_loader, desc="Training", leave=False)

        for batch in train_loop:
            inputs, labels = [b.to(device) for b in batch]
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            train_loop.set_postfix(loss=loss.item())

        avg_train_loss = total_loss / len(train_loader)

        model.eval()
        correct, total = 0, 0
        val_loop = tqdm(val_loader, desc="Validating", leave=False)

        with torch.no_grad():
            for batch in val_loop:
                inputs, labels = [b.to(device) for b in batch]
                outputs = model(inputs)
                preds = torch.round(torch.sigmoid(outputs))
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        val_acc = correct / total
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_acc)

        print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc:.4f}")

    # Plotting
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_accuracies, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.legend()
    plt.title('Training Loss vs. Validation Accuracy')
    plt.show()

train_model(model, train_loader, val_loader, epochs = 100)

from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
import numpy as np

def evaluate(model, data_loader):
    model.eval()
    all_labels, all_probs = [], []

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = torch.sigmoid(outputs)
            all_labels.append(labels.cpu().numpy())
            all_probs.append(probs.cpu().numpy())

    y_true = np.concatenate(all_labels)
    y_prob = np.concatenate(all_probs)
    y_pred = (y_prob >= 0.5).astype(int)

    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0)
    auc = roc_auc_score(y_true, y_prob)

    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc:.4f}")
    return y_true, y_pred

y_true, y_pred = evaluate(model, val_loader)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Legit', 'Fake'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

torch.save(model.state_dict(), "fake_job_model.pth")